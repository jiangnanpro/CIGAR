{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f4fadd3-7e12-4f61-9d2f-2971cc5b426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74cc1e47-c6b7-4228-8540-8f78f43d6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff8a0123-fc62-40ba-8c07-86c8c3356ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_action_name = (\n",
    "                pd.read_csv(DATA_DIR / 'action_features_train.csv.gz', index_col = [0])\n",
    "                .sort_values(by=['names_number'],ascending=False)\n",
    "                .head(100)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22be8fb5-e7ad-46e7-b75d-b0a4058e138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# initialize the RoBERTa based model. \n",
    "\n",
    "import torch\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']\n",
    "\n",
    "config = config_class.from_pretrained(\"roberta-base\")\n",
    "tokenizer = tokenizer_class.from_pretrained(\"roberta-base\")\n",
    "encoder = model_class.from_pretrained(\"roberta-base\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12eee97a-5fae-4f42-ac87-94cb5a023e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_tokenization(phrase, tokenizer, max_token_length):\n",
    "    \n",
    "    '''This function tokenize the phrases into ids'''\n",
    "    \n",
    "    phrase_tokens = tokenizer.tokenize(phrase)\n",
    "    phrase_tokens = phrase_tokens[:max_token_length-2]\n",
    "    phrase_tokens = [tokenizer.cls_token]+phrase_tokens+[tokenizer.sep_token]\n",
    "    \n",
    "    phrase_ids=tokenizer.convert_tokens_to_ids(phrase_tokens)\n",
    "    padding_length = max_token_length - len(phrase_ids)\n",
    "    phrase_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    return phrase_ids\n",
    "\n",
    "def phrases_tokenization(list_phrases, tokenizer, encoder, max_token_length = 128):\n",
    "    \n",
    "    '''This function tansfer phrases' ids into tensors and apply our model's encoder to embed the phrases' ids into high-dimensional vectors'''\n",
    "    \n",
    "    max_token_length = max_token_length\n",
    "    #print(max_token_length)\n",
    "    \n",
    "    ##### -----------> Here is a problem: what about the descriptions with more than 25 words?\n",
    "\n",
    "    \n",
    "    phrases_tokens = [phrase_tokenization(phrase, tokenizer, max_token_length) for phrase in list_phrases]\n",
    "    \n",
    "    #phrases_tensors = [torch.tensor(token) for token in phrases_tokens]\n",
    "    \n",
    "    #phrases_embeddings = [encoder.encode(input_ids=phrase.unsqueeze(0)) for phrase in tqdm(phrases_tensors, position=0, leave=True)]\n",
    "    \n",
    "\n",
    "    return phrases_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d103b951-9b60-474a-9dc4-4284d1dc6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract test data from dataframe 'df_action_name'.\n",
    "\n",
    "list_actions = df_action_name.to_dict('records')\n",
    "\n",
    "list_names = [str(action['name_official']).lower() for action in list_actions]\n",
    "list_descriptions = [str(action['description_official']).lower() for action in list_actions]\n",
    "list_names_users = [list(action['names_users'].lower().split(',')) for action in list_actions]\n",
    "#list_names_users = [list(set(action['names_users'].lower().split(','))) for action in list_actions]\n",
    "\n",
    "list_names_users = [[name.strip() for name in names] for names in list_names_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6f13fd-f750-41af-bede-695404514b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 most frequently applied GitHub Actions has been selected! \n",
      "\n",
      "e.g. Action number 0: \n",
      "\n",
      "Official name: \t\t upload a build artifact\n",
      "Official description:\t upload a build artifact that can be used by subsequent workflow steps \n",
      "\n",
      "With 2944 User-assigned names, give 10 examples:\n",
      "['upload wheelhouse artifact',\n",
      " 'upload pytest log as artifact',\n",
      " 'ðŸ“¤ upload artifact: html',\n",
      " 'archive artifacts (geyser velocity)',\n",
      " 'upload binary files (linux_arm64)',\n",
      " 'upload entitygraphql.aspnet',\n",
      " 'upload test results on failure',\n",
      " 'upload clang-format patch as artifact',\n",
      " 'archive jemalloc binary artifact [centos-8] to github',\n",
      " 'upload test result artifact']\n"
     ]
    }
   ],
   "source": [
    "# show data.\n",
    "\n",
    "print(f'Top {len(list_names)} most frequently applied GitHub Actions has been selected! \\n' )\n",
    "n_example = 0\n",
    "print(f'e.g. Action number {n_example}: \\n')\n",
    "print(f'Official name: \\t\\t {list_names[n_example]}')\n",
    "print(f'Official description:\\t {list_descriptions[n_example]} \\n')\n",
    "print(f'With {len(list_names_users[n_example])} User-assigned names, give 10 examples:')\n",
    "pprint.pprint(list_names_users[n_example][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1e492-e5ae-4208-8ea3-0e36794b3b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
