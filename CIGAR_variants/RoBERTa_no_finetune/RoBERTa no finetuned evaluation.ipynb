{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0638e103-530f-4cff-914c-d35536095b6e",
   "metadata": {},
   "source": [
    "# In this notebook we test and evaluate RoBERTa without finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d77845e-8127-446d-b065-c6cabb44ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to data folder\n",
    "\n",
    "DATA_DIR = Path('../../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397e266-e0d1-4013-942b-8cf66447b348",
   "metadata": {},
   "source": [
    "# Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4d1614-248c-49de-9714-ccd092d16dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>name_official</th>\n",
       "      <th>description_official</th>\n",
       "      <th>names_users</th>\n",
       "      <th>names_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actions/upload-artifact</td>\n",
       "      <td>Upload a Build Artifact</td>\n",
       "      <td>Upload a build artifact that can be used by su...</td>\n",
       "      <td>archive ruby package artifact [ruby-pkg_3.1.0_...</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actions/cache</td>\n",
       "      <td>Cache</td>\n",
       "      <td>Cache artifacts like dependencies and build ou...</td>\n",
       "      <td>cache gdcm,load cached .local,restore node_mod...</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actions/checkout</td>\n",
       "      <td>Checkout</td>\n",
       "      <td>Checkout a Git repository at a particular version</td>\n",
       "      <td>checkout amplitude-ios gh-pages for building d...</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>actions/download-artifact</td>\n",
       "      <td>Download a Build Artifact</td>\n",
       "      <td>Download a build artifact that was previously ...</td>\n",
       "      <td>download vs2022 uwp lite,fetch artifacts for a...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>actions/upload-release-asset</td>\n",
       "      <td>Upload a Release Asset</td>\n",
       "      <td>Upload a release asset to an existing release ...</td>\n",
       "      <td>upload archlinux package x86_64,upload plugin-...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>juliangruber/read-file-action</td>\n",
       "      <td>Read file</td>\n",
       "      <td>Read file contents</td>\n",
       "      <td>read package.json,read the pr_num file,read sd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>aquasecurity/trivy-action</td>\n",
       "      <td>Aqua Security Trivy</td>\n",
       "      <td>Scans container images for vulnerabilities wit...</td>\n",
       "      <td>trivy scan,run trivy on deployment.yaml,trivy ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>r0adkll/sign-android-release</td>\n",
       "      <td>Sign Android release</td>\n",
       "      <td>An action to sign an Android release APK or AAB</td>\n",
       "      <td>sign app apk,sign debug apk,sign premium apk,s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>crazy-max/ghaction-docker-meta</td>\n",
       "      <td>Docker Metadata action</td>\n",
       "      <td>GitHub Action to extract metadata (tags, label...</td>\n",
       "      <td>get docker tags,docker tomcat meta,gather dock...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>maxim-lobanov/setup-xcode</td>\n",
       "      <td>Setup Xcode version</td>\n",
       "      <td>Set up your GitHub Actions workflow with a spe...</td>\n",
       "      <td>select latest xcode,setup xcode version,select...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            action              name_official  \\\n",
       "0          actions/upload-artifact    Upload a Build Artifact   \n",
       "1                    actions/cache                      Cache   \n",
       "2                 actions/checkout                   Checkout   \n",
       "3        actions/download-artifact  Download a Build Artifact   \n",
       "4     actions/upload-release-asset     Upload a Release Asset   \n",
       "..                             ...                        ...   \n",
       "92   juliangruber/read-file-action                  Read file   \n",
       "90       aquasecurity/trivy-action        Aqua Security Trivy   \n",
       "89    r0adkll/sign-android-release       Sign Android release   \n",
       "88  crazy-max/ghaction-docker-meta     Docker Metadata action   \n",
       "99       maxim-lobanov/setup-xcode        Setup Xcode version   \n",
       "\n",
       "                                 description_official  \\\n",
       "0   Upload a build artifact that can be used by su...   \n",
       "1   Cache artifacts like dependencies and build ou...   \n",
       "2   Checkout a Git repository at a particular version   \n",
       "3   Download a build artifact that was previously ...   \n",
       "4   Upload a release asset to an existing release ...   \n",
       "..                                                ...   \n",
       "92                                 Read file contents   \n",
       "90  Scans container images for vulnerabilities wit...   \n",
       "89    An action to sign an Android release APK or AAB   \n",
       "88  GitHub Action to extract metadata (tags, label...   \n",
       "99  Set up your GitHub Actions workflow with a spe...   \n",
       "\n",
       "                                          names_users  names_number  \n",
       "0   archive ruby package artifact [ruby-pkg_3.1.0_...           736  \n",
       "1   cache gdcm,load cached .local,restore node_mod...           363  \n",
       "2   checkout amplitude-ios gh-pages for building d...           359  \n",
       "3   download vs2022 uwp lite,fetch artifacts for a...           228  \n",
       "4   upload archlinux package x86_64,upload plugin-...           218  \n",
       "..                                                ...           ...  \n",
       "92  read package.json,read the pr_num file,read sd...             4  \n",
       "90  trivy scan,run trivy on deployment.yaml,trivy ...             4  \n",
       "89  sign app apk,sign debug apk,sign premium apk,s...             4  \n",
       "88  get docker tags,docker tomcat meta,gather dock...             4  \n",
       "99  select latest xcode,setup xcode version,select...             4  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data from action_test.cev.gz file\n",
    "\n",
    "df_action_name = (\n",
    "    pd.read_csv(DATA_DIR / 'test.csv.gz', index_col = [0])\n",
    "    .sort_values(by=['names_number'],ascending=False)\n",
    "    .head(100)\n",
    ")\n",
    "\n",
    "df_action_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cbee2f-450e-4113-856d-24aa09ec1461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 22:49:11.406351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-05 22:49:16.586701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize the RoBERTa based model. \n",
    "\n",
    "import torch\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaModel, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']\n",
    "\n",
    "config = config_class.from_pretrained(\"roberta-base\")\n",
    "tokenizer = tokenizer_class.from_pretrained(\"roberta-base\")\n",
    "encoder = model_class.from_pretrained(\"roberta-base\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f971feb1-023f-40be-a6ea-b28323bd6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model's parameters from checkpoint.\n",
    "\n",
    "from model import Model\n",
    "\n",
    "#input_dir = './saved_models/action_model_finetuned/checkpoint-best-f1/model.bin'\n",
    "\n",
    "args = ''\n",
    "\n",
    "encoder=Model(encoder,config,tokenizer,args)\n",
    "#encoder.load_state_dict(torch.load(input_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d149921-7772-4e66-80ab-fcfaaec4a74c",
   "metadata": {},
   "source": [
    "# Text embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959680d6-5ae5-49ef-9652-06ebf9908fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_tokenization(phrase, tokenizer, max_token_length):\n",
    "    \n",
    "    '''This function tokenize the phrases into ids'''\n",
    "    \n",
    "    phrase_tokens = tokenizer.tokenize(phrase)\n",
    "    phrase_tokens = phrase_tokens[:max_token_length-2]\n",
    "    phrase_tokens = [tokenizer.cls_token]+phrase_tokens+[tokenizer.sep_token]\n",
    "    \n",
    "    phrase_ids=tokenizer.convert_tokens_to_ids(phrase_tokens)\n",
    "    padding_length = max_token_length - len(phrase_ids)\n",
    "    phrase_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    return phrase_ids\n",
    "\n",
    "def phrases_embedding(list_phrases, tokenizer, encoder, max_token_length = 128):\n",
    "    \n",
    "    '''This function tansfer phrases' ids into tensors and apply our model's encoder to embed the phrases' ids into high-dimensional vectors'''\n",
    "    \n",
    "    max_token_length = max_token_length\n",
    "    \n",
    "    phrases_tokens = [phrase_tokenization(phrase, tokenizer, max_token_length) for phrase in list_phrases]\n",
    "    phrases_tensors = [torch.tensor(token) for token in phrases_tokens]\n",
    "    phrases_embeddings = [encoder.encode(input_ids=phrase.unsqueeze(0)) for phrase in tqdm(phrases_tensors, position=0, leave=True)]\n",
    "    \n",
    "\n",
    "    return phrases_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f059f6-13a1-452b-80cd-ddaad52b3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract test data from dataframe 'df_action_name'.\n",
    "\n",
    "list_actions = df_action_name.to_dict('records')\n",
    "\n",
    "\n",
    "list_names = [str(action['name_official']).lower() for action in list_actions]\n",
    "list_descriptions = [str(action['description_official']).lower() for action in list_actions]\n",
    "list_names_users = [list(action['names_users'].lower().split(',')) for action in list_actions]\n",
    "\n",
    "list_names_users = [[name.strip() for name in names] for names in list_names_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c279ae0-276d-42d7-a770-f14142362246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 most frequently applied GitHub Actions has been selected! \n",
      "\n",
      "e.g. Action number 0: \n",
      "\n",
      "Official name: \t\t upload a build artifact\n",
      "Official description:\t upload a build artifact that can be used by subsequent workflow steps \n",
      "\n",
      "With 736 User-assigned names, give 10 examples:\n",
      "['archive ruby package artifact [ruby-pkg_3.1.0_ubuntu-20.04_malloctrim] to '\n",
      " 'github',\n",
      " 'upload source artifact',\n",
      " 'upload built site as artifacts',\n",
      " 'upload diff output',\n",
      " 'upload intellij build',\n",
      " 'archive artifacts (floodgate velocity)',\n",
      " 'store tarball as artifact',\n",
      " 'store source distribution',\n",
      " 'upload build mapping',\n",
      " 'storing test data artifacts']\n"
     ]
    }
   ],
   "source": [
    "# show data.\n",
    "\n",
    "print(f'Top {len(list_names)} most frequently applied GitHub Actions has been selected! \\n' )\n",
    "n_example = 0\n",
    "print(f'e.g. Action number {n_example}: \\n')\n",
    "print(f'Official name: \\t\\t {list_names[n_example]}')\n",
    "print(f'Official description:\\t {list_descriptions[n_example]} \\n')\n",
    "print(f'With {len(list_names_users[n_example])} User-assigned names, give 10 examples:')\n",
    "pprint.pprint(list_names_users[n_example][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d057deeb-5405-4b72-bdb4-144c7f9c7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# embed test data. (official names + official descriptions)\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "names_official_embeddings = phrases_embedding(list_names, tokenizer, encoder, block_size)\n",
    "descriptions_official_embeddings = phrases_embedding(list_descriptions, tokenizer, encoder, block_size)\n",
    "\n",
    "# different way to aggregate the embedding of names and descriptions: Arithmetic mean & L2-Norm\n",
    "\n",
    "combined_official_embeddings = [torch.div((name+description),2) for name,description in zip(names_official_embeddings,descriptions_official_embeddings)]\n",
    "combined_official_embeddings_l2 = [torch.sqrt(torch.square(name)+torch.square(description)) for name,description in zip(names_official_embeddings,descriptions_official_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf4ba8d-d3b9-42ab-a86f-9784c20d72fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 10.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  8.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# embed test data. (user-assigned names)\n",
    "\n",
    "list_names_users_embeddings = [phrases_embedding(name[:10],tokenizer, encoder, block_size) for name in list_names_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f93e60-befe-4bff-bc91-36f702b4428b",
   "metadata": {},
   "source": [
    "# Model testing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38a6748-ddb1-471c-b11d-94482f6f2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(list_names_users_embeddings,official_embeddings,top_n = 3):\n",
    "    \n",
    "    '''\n",
    "    This function calculates the cosine similarities between each embedding vector of the user-assigned names \n",
    "    and every vectors of official name/description, etc., then sorts the results from the highest value to the lowest value\n",
    "    along with their indexes(labels), finally returns the top_n indexes(labels) as the output suggested_labels.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    predicted_labels = []\n",
    "\n",
    "    for names_users in tqdm(list_names_users_embeddings, position=0, leave=True):\n",
    "        predicted_label= []\n",
    "\n",
    "        for name_user in names_users:\n",
    "            #print(name_user.shape)\n",
    "            #print(names_official_embeddings[0].shape)\n",
    "            cos_sim = [[F.cosine_similarity(official[0].view(1,-1).float(), name_user.view(1,-1).float()),index] for index,official in enumerate(official_embeddings)]\n",
    "            \n",
    "            predicted_label.append(sorted(cos_sim,reverse=True)[:top_n])\n",
    "\n",
    "\n",
    "        predicted_labels.append(predicted_label)\n",
    "        \n",
    "    suggested_labels = [[[label[1] for label in labels] for labels in one_predicted_labels] for one_predicted_labels in predicted_labels]\n",
    "    \n",
    "    return suggested_labels\n",
    "\n",
    "\n",
    "def predict_cs(list_names_users_embeddings,names_official_embeddings,descriptions_official_embeddings,top_n = 3):\n",
    "    \n",
    "    '''\n",
    "    This function is slightly different from the previous predict function. It combines the cos_similarities calculated from \n",
    "    (user-assigned names, official names) & (user-assigned names, descriptions) by taking the arithmetic mean value.\n",
    "    '''\n",
    "    \n",
    "    predicted_labels = []\n",
    "\n",
    "    for names_users in tqdm(list_names_users_embeddings, position=0, leave=True):\n",
    "        predicted_label= []\n",
    "\n",
    "        for name_user in names_users:\n",
    "            cos_sim_name = [F.cosine_similarity(name_official[0].view(1,-1).float(), name_user.view(1,-1).float()) for name_official in names_official_embeddings]\n",
    "            cos_sim_description = [F.cosine_similarity(description_official[0].view(1,-1).float(), name_user.view(1,-1).float()) for description_official in descriptions_official_embeddings]\n",
    "\n",
    "            divide = lambda x,y: (x+y) /2\n",
    "            cos_sim = [[divide(cos_sim_name[index],cos_sim_description[index]),index] for index in range(len(cos_sim_name))]\n",
    "            \n",
    "            predicted_label.append(sorted(cos_sim,reverse=True)[:top_n])\n",
    "\n",
    "\n",
    "        predicted_labels.append(predicted_label)\n",
    "        \n",
    "    suggested_labels = [[[label[1] for label in labels] for labels in one_predicted_labels] for one_predicted_labels in predicted_labels]\n",
    "    \n",
    "    return suggested_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f167382e-070f-4f2b-bc4a-803d5c07d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_rate_(predicted_labels, top_n):\n",
    "    '''\n",
    "    This function calculates the coverage rate of the model's suggestions.\n",
    "    '''\n",
    "    covered_names = []\n",
    "\n",
    "    for action in predicted_labels:\n",
    "        all_name_per_action = []\n",
    "        for name in action:\n",
    "            all_name_per_action += name[:top_n]\n",
    "        \n",
    "        all_name_per_action = list(set(all_name_per_action))\n",
    "\n",
    "        covered_names += all_name_per_action\n",
    "    \n",
    "    covered_names = list(set(covered_names))\n",
    "    coverage = round(len(covered_names)/100,3)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "def find_false_cases(predicted_labels, top_n):\n",
    "    '''\n",
    "    This function finds the failed cases, i.e. the expected action is not been suggested by the model.\n",
    "    '''\n",
    "    false_cases = []\n",
    "    for action_index,predicted_labels_action in enumerate(predicted_labels):\n",
    "        \n",
    "        false_cases_action = []\n",
    "    \n",
    "        for name_index, labels in enumerate(predicted_labels_action):\n",
    "            if action_index not in labels[:top_n]:\n",
    "                false_cases_action.append([action_index,name_index])\n",
    "\n",
    "        if false_cases_action:\n",
    "            false_cases.append(false_cases_action)\n",
    "    \n",
    "    return false_cases\n",
    "    \n",
    "def overall_success_rate(suggested_labels, prediction_type, top_n):\n",
    "    '''\n",
    "    This function calculates the average success rate of the model, and prints it.\n",
    "    '''\n",
    "    success = 0\n",
    "\n",
    "    for action_index,predicted_labels in enumerate(suggested_labels):\n",
    "        success += len([n for n in predicted_labels if action_index in n[:top_n]])\n",
    "\n",
    "    number_names = sum([len(suggestion) for suggestion in suggested_labels])\n",
    "    average_acc = round(success/number_names,3)\n",
    "    print(prediction_type+'.')\n",
    "\n",
    "    return average_acc\n",
    "\n",
    "def count_number(list_samples):\n",
    "    '''\n",
    "    This function counts the overall number of samples in a list of list.\n",
    "    It's been used to count the failed samples and also the number of samples existing in test set.\n",
    "    '''\n",
    "    return sum([len(samples) for samples in list_samples])\n",
    "\n",
    "\n",
    "def output_to_df(list_names_users, suggested_labels):\n",
    "\n",
    "    list_selected_user_names = [name[:10] for name in list_names_users]\n",
    "\n",
    "    list_all_output = []\n",
    "    for action_index,name in enumerate(list_selected_user_names):\n",
    "        for n in range(len(name)):\n",
    "            dict_output = {}\n",
    "            dict_output['name'] = name[n]\n",
    "            dict_output['actual_action'] = action_index\n",
    "            dict_output['suggested_actions'] = suggested_labels[action_index][n]\n",
    "            list_all_output.append(dict_output)\n",
    "    \n",
    "    df_output = pd.DataFrame.from_records(list_all_output)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fa2aa4-7cde-47ad-af2d-c5a1caa792b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:41<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute the suggested labels in different ways.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "suggested_labels_combined_embedding = predict(list_names_users_embeddings, combined_official_embeddings,top_n)\n",
    "#suggested_labels_combined_cs = predict_cs(list_names_users_embeddings,names_official_embeddings,descriptions_official_embeddings,top_n)\n",
    "#suggested_labels_name = predict(list_names_users_embeddings,names_official_embeddings,top_n)\n",
    "#suggested_labels_description = predict(list_names_users_embeddings,descriptions_official_embeddings,top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96fff3ad-fae7-4022-b044-bb80915be7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both w/ vector mean.\n",
      "Success rate: 0.19\n",
      "Coverage rate: 0.95\n",
      "Failure: 100/100 actions has at least one failed suggestion; 635/784 fail attempts in total.\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "suggested_labels = suggested_labels_combined_embedding\n",
    "prediction_type = 'Both w/ vector mean'\n",
    "#prediction_type = 'Both w/ mean similarities'\n",
    "#prediction_type = 'Names only'\n",
    "#prediction_type = 'Description only'\n",
    "\n",
    "success_rate = overall_success_rate(suggested_labels, prediction_type, top_n)\n",
    "coverage_rate = coverage_rate_(suggested_labels,top_n)\n",
    "false_cases = find_false_cases(suggested_labels,top_n)\n",
    "\n",
    "n_fail = count_number(false_cases)\n",
    "n_predict = count_number([name[:10] for name in list_names_users])\n",
    "\n",
    "print(f'Success rate: {success_rate}')\n",
    "print(f'Coverage rate: {coverage_rate}')\n",
    "print(f'Failure: {len(false_cases)}/100 actions has at least one failed suggestion; {n_fail}/{n_predict} fail attempts in total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b095f3-9de7-465e-bfc5-55fe973f7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the output results into .csv files.\n",
    "\n",
    "df_output = output_to_df(list_names_users, suggested_labels)\n",
    "df_output.to_csv('../results/results_t'+str(Tau)+'.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad70f4-b11f-45fb-babd-46d53fd28b17",
   "metadata": {},
   "source": [
    "# Failure investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ae1f12a-bd65-412b-a1b3-321921dffa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(name, tokenizer, names_official_embeddings, list_actions, max_token_length = 128, top_n = 3):\n",
    "    \n",
    "    name_tokens = phrase_tokenization(name, tokenizer, max_token_length)\n",
    "    #print(name_tokens)\n",
    "    name_tensors = torch.tensor(name_tokens)\n",
    "    name_embedding = encoder.encode(input_ids=name_tensors.unsqueeze(0))\n",
    "    #print(name_embedding.shape)\n",
    "    \n",
    "\n",
    "    cos_sim = [[F.cosine_similarity(name_official[0].view(1,-1).float(), name_embedding.view(1,-1).float()),index] for index,name_official in enumerate(names_official_embeddings)]\n",
    "            \n",
    "    predicted_label = sorted(cos_sim,reverse=True)[:top_n]\n",
    "\n",
    "    suggested_labels = [labels[1] for labels in predicted_label]\n",
    "    \n",
    "    print(f'Top {top_n} suggested GitHub Actions:')\n",
    "    \n",
    "    for i in range(len(suggested_labels)):\n",
    "        print(f'{i}: {list_actions[suggested_labels[i]][\"action\"]}')\n",
    "    \n",
    "    return \n",
    "\n",
    "#predict_single('Download a Build Artifact',tokenizer,names_official_embeddings,list_actions,128,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20feffd2-faa3-433a-b596-b14aae760618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[6, 2], [6, 9]],\n",
       " [[7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 6], [7, 7], [7, 8], [7, 9]],\n",
       " [[8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 8], [8, 9]],\n",
       " [[10, 0], [10, 7]],\n",
       " [[11, 8]],\n",
       " [[12, 0]],\n",
       " [[13, 2], [13, 7], [13, 9]],\n",
       " [[14, 0],\n",
       "  [14, 1],\n",
       "  [14, 2],\n",
       "  [14, 3],\n",
       "  [14, 4],\n",
       "  [14, 5],\n",
       "  [14, 6],\n",
       "  [14, 8],\n",
       "  [14, 9]],\n",
       " [[15, 0]],\n",
       " [[16, 0]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_cases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6396ed43-8ada-4934-af96-e8febc15db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One failure example(user-assigned name):'upload source artifact'\n",
      "\n",
      "Expect action: actions/upload-artifact\n",
      "\n",
      "Top 5 suggested GitHub Actions:\n",
      "0: actions/upload-artifact\n",
      "1: actions/upload-release-asset\n",
      "2: brandedoutcast/publish-nuget\n",
      "3: drusellers/publish-nuget\n",
      "4: jakejarvis/s3-sync-action\n"
     ]
    }
   ],
   "source": [
    "n_action, n_name = 0,1\n",
    "\n",
    "print(f\"One failure example(user-assigned name):'{list_names_users[n_action][n_name]}'\\n\")\n",
    "print(f\"Expect action: {list_actions[n_action]['action']}\\n\")\n",
    "\n",
    "predict_single(list_names_users[0][1],tokenizer,names_official_embeddings,list_actions,128,top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9c068-0d3b-4d01-b6f5-40454d988e2c",
   "metadata": {},
   "source": [
    "# Vector visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "479c3d64-4521-440a-ad95-9679f59add73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 16/16 [00:03<00:00,  4.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  3.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:02<00:00,  6.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00,  9.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00,  7.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  8.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  8.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  8.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  8.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  8.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  8.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  8.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  8.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  6.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  6.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  7.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  8.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  8.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  8.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  8.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  8.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  8.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  8.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  8.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  7.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  9.18it/s]\n"
     ]
    }
   ],
   "source": [
    "list_names_users_visualization = [phrases_embedding(name[:30],tokenizer, encoder, block_size) for name in list_names_users[30:60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49445f8e-0311-41a1-ade9-8efbe151e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data_visualization = list_names_users_visualization[9:14]\n",
    "group_tensors = [torch.cat(inner_list, dim=0) for inner_list in data_visualization]\n",
    "\n",
    "with torch.no_grad():\n",
    "    flattened_data = torch.cat(group_tensors, dim=0)\n",
    "    flattened_data = flattened_data.view(flattened_data.size(0), -1).numpy()\n",
    "\n",
    "labels = np.repeat(np.arange(len(data_visualization)), [len(inner_list) for inner_list in data_visualization])\n",
    "\n",
    "\n",
    "# Perform t-SNE dimension reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_data = tsne.fit_transform(flattened_data)\n",
    "\n",
    "# plot the embedded data with different colors for each group\n",
    "for group_idx in np.unique(labels):\n",
    "    group_mask = (labels == group_idx)\n",
    "    plt.scatter(embedded_data[group_mask, 0], embedded_data[group_mask, 1], label=f\"{list_actions[39+group_idx]['name_official']}\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66ac65-54a6-4cd8-b255-6f9f1a619a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
